\apendice{Documentación técnica de programación}

\section{Introducción}

El proyecto, al estar descompuesto en microservicios, tiene varias partes bien diferenciadas.

\section{Estructura de directorios}


\subsection{Aplicación web}
La aplicación web tiene una estructura de directorios algo compleja, por lo que algunos elementos se han omitido del esquema.

\begin{tabbing}
\hphantom{tab }\= \hphantom{tab }\= \hphantom{tab }\= \hphantom{tab }\= \hphantom{quadruple tabula}\= \kill\\
WhatAClass/ \> \> \> \> \> \textit{Principal localización para nuestro código} \\
\> \> \> \> \> \textit{fuente}\\
\> blueprints/ \> \> \> \> \textit{Localización de los controladores} \\
\> \> oauth/ \> \> \> \textit{Controladores de OAuth} \\
\> static/ \> \> \> \> \textit{Archivos estáticos (bootstrap y favicon)}\\
\> templates/ \> \> \> \> \textit{Plantillas de Jinja2 para generar html o} \\ 
\> \> \> \> \> \textit{javascript} \\
\> \> tensorflow\_mng/ \> \> \> \textit{Plantillas para tensorflow}\\
\> \> user\_mng/\> \> \> \textit{Plantillas para el control de usuarios}\\
\> \> \> email/ \> \> \textit{Plantillas para los email}\\
\> translations/ \> \> \> \> \textit{Mensajes traducidos (compilación)} \\
\> utils/ \> \> \> \> \textit{Utilidades} \\
\> \> email.py \> \> \> \textit{Utilidad de email}\\
\> app.py \> \> \> \> \textit{Builder}\\
\> extensions.py \> \> \> \> \textit{Extesiones de flask}\\
\> forms.py \> \> \> \> \textit{Formularios}\\
\> models.py \> \> \> \> \textit{Modelos}\\
Alembic/ \> \> \> \> \> \textit{Versionado de la DB}\\
\> versions/ \> \> \> \> \textit{Scripts de migración entre versiones de la DB}\\
babel/ \> \> \> \> \> \textit{Mensajes con sus traducciones}\\
config/ \> \> \> \> \> \textit{Configuración de la aplicación}\\
\> default.py \> \> \> \> \textit{Configuración por defecto}\\
docs/ \> \> \> \> \> \textit{Documentación autogenerada}\\
report/ \> \> \> \> \> \textit{Documentación manual}\\
resources/\> \> \> \> \> \textit{Recursos como claves rsa}\\
tests/ \> \> \> \> \> \textit{Tests de la aplicación}\\
.coveragerc \> \> \> \> \> \textit{Que código se ignora por el cubrimiento}\\
.dockerignore \> \> \> \> \> \textit{Que ignora docker}\\
.gitignore \> \> \> \> \> \textit{Que ignora git}\\
.travis.yml \> \> \> \> \> \textit{Configuración de travis}\\
babel.cfg \> \> \> \> \> \textit{Configuración de babel (internacionalización)}\\
create\_db.py \> \> \> \> \> \textit{Script de creación de la DB a partir del modelo}\\
docker-compose.yml \> \> \> \> \> \textit{Configuración de docker compose (solo webapp)}\\
Dockerfile \> \> \> \> \> \textit{Descripción de construcción del contenedor}\\
LICENSE \> \> \> \> \> \textit{Licencia}\\
messages.pot \> \> \> \> \> \textit{Mensajes traducidos}\\
Procfile \> \> \> \> \> \textit{Comandos para heroku}\\
README.md \> \> \> \> \> \textit{Readme para Github}\\
requirements.txt \> \> \> \> \> \textit{Requisitos para desarrollador y para VersionEye}\\
requirements-prod.txt \> \> \> \> \> \textit{Requisitos para despliegue}\\
run.py \> \> \> \> \> \textit{Punto de entrada para debug} \\
runtime.txt \> \> \> \> \> \textit{Establece la versión de python para heroku}\\
setup.cfg \> \> \> \> \> \textit{Alias para test}\\
setup.py \> \> \> \> \> \textit{Instalación de las dependencias}\\
start.py \> \> \> \> \> \textit{Punto de entrada sin configuración adicional}\\
test.py \> \> \> \> \> \textit{Punto de entrada para los test}\\
uwsgi.ini  \> \> \> \> \> \textit{Configuración para que uwsgi encuentra la aplicación}\\
wsgi.py  \> \> \> \> \> \textit{Punto de entrada para uwsgi}\\
\end{tabbing}

\subsection{Microservicio de tensorflow}
\begin{tabbing}

\hphantom{tab tab tab tab tab tab tab tab }\= \kill\\
dataset/ \> \textit{Estructura de archivos en producción para facilitar} \\
\> \textit{los test} \\
graphs/ \> \textit{Estructura de archivos de producción para los test} \\
images/ \> \textit{Estructura de archivos de producción para los test} \\
test/ \> \textit{Carpeta donde se guardan los test} \\
Dockerfile \> \textit{Descripción de construcción del contenedor} \\
fit\_dataset.py \> \textit{Script para ajustar el modelo a un dataset} \\
id\_rsa.pub \> \textit{Clave rsa para comunicación con la aplicación web} \\
inceptionv3* \> \textit{Modelo y clases} \\
maybe\_start\_retrain.py \> \textit{Script que comprueba si la petición ya se está} \\ 
\> \textit{ejecutando o no} \\
run\_inference.py \> \textit{Script para inferir la clase de una imagen} \\
\end{tabbing}

\subsection{Docker compose}
Hay dos archivos que se han hecho fuera de las dos anteriores y estos son los docker-compose.yml que coordinan ambos servicios. 

Uno está en la raíz de ambas estructuras, este hace la construcción del servicio contando con los fuentes.

El otro que está en docker-compose-hub/docker-compose.yml, y este usa los contenedores disponibles en dockerhub en vez de construirlos desde los fuentes.


\section{Manual del programador}

En la siguiente sección se expondrá como se puede continuar desarrollando la aplicación.

\subsection{Entorno de desarrollo}

Los siguientes elementos son necesarios para trabajar en el proyecto de forma efectiva:

\begin{itemize}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\item Python 3.5 o 3.6
\item Git
\item Gestor de paquetes
\item Entorno virtual
\item IDE o editor de texto
\item Docker
\end{itemize}

\subsection{Python}

El lenguaje de programación más popular en este momento, tanto para web, como para aplicaciones de empresa es Python \cite{ppl}.

El proyecto se inició con Python 3.5 aunque se ha mantenido la compatibilidad con Python 3.6. Debido a algunos cambios que se han hecho en Python 3.6 con los diccionarios, se recomienda esta versión, ya que proporciona un aumento de rendimiento.

Se puede obtener esta versión (3.6) en \cite{py}. Hay que elegir correctamente la versión y sistema operativo. Después de seguir el manual de instalación tendremos Python instalado.

\subsection{Git}

La forma más simple y útil de descargar y trabajar con el código fuente es a través de un sistema de control de versiones como es Git \cite{git}.

Este programa permite movernos a través de la evolución que ha tenido el código a lo largo del tiempo.

Una vez lo tengamos instalado, tenemos dos posibilidades: usar un soporte con GUI o usar un soporte en línea de comandos, por supuesto hay algunos programas que lo instalan automáticamente.

De la primera opción están el proporcionado por Git en algunas instalaciones, los plugin o utilidades para IDEs o un programa especializado como GitKraken.

La segunda opción es la que se recomienda porque ayuda a comprender el funcionamiento de Git de una manera más clara.


\subsection{Gestor de paquetes}

El gestor de paquetes es una utilidad considerada necesaria para facilitar la instalación de paquetes (en este contexto significa un conjunto de software), ya que no solo instalamos el código fuente que pedimos sino que también instalamos las dependencias de dicho código.

Las principales herramientas son \hfoot{https://pypi.python.org/pypi/pip}{Pip}, \hfoot{https://pypi.python.org/pypi/setuptools}{Setuptools} y \hfoot{https://pypi.python.org/pypi/wheel}{Wheel}.

Si hemos instalado Python 2 con la versión 2.7.9+ o Python 3 con la versión 3.4+ ya tienen instalado pip y setuptools.

En linux, ya que el sistema operativo tiene muchas veces tanto la versión 2.x y 3.x de Python para usar pip tendremos que usar pip2 o pip3 para saber que versión específica se está usando.

\subsubsection{Uso básico de pip}

Para instalar un proyecto específico con pip \cite{pip} se hace con:

\begin{lstlisting}[language=bash]
    $ pip install <PROJECT>
\end{lstlisting}

Algunas opciones adicionales son las versiones específicas o compatibles:

\begin{lstlisting}[language=bash, escapechar=\%]
    $ pip install <PROJECT>==<SPECIFIC>
    $ pip install <PROJECT>%$\sim$%=<COMPATIBLE>
\end{lstlisting}

Para actualizar un proyecto:

\begin{lstlisting}[language=bash]
    $ pip install --upgrade <PROJECT>
\end{lstlisting}

Por último, cabe destacar que para un conjunto de dependencias la manera más simple de instalar todas es tenerlas en un archivo requirements.txt, para instalar a partir de ese archivo se usa el siguiente comando:

\begin{lstlisting}[language=bash]
    $ pip install -r requirements.txt
\end{lstlisting}

Para obtener este archivo el desarrollador anterior debe haber generado ese archivo usando pip, esto descargará las dependencias que tiene el entorno desde el que se ha generado.

\begin{lstlisting}[language=bash]
    $ pip freeze
\end{lstlisting}



\subsection{Entorno virtual}

Usar un entorno virtual no es una necesidad a la hora de desarrollar programas en Python, pero se recomienda encarecidamente debido a que eso nos permite encapsular nuestras dependencias. La excepción a la regla anterior es el caso de desarrollar dos programas con dependencias a las mismas bibliotecas en diferentes versiones.

Algunos de los gestores de entornos virtuales más usados son venv, virtualenv y conda. Hay que tener en cuenta que los IDEs necesitan configuración adicional para trabajar con entornos virtuales. 

Las características principales de las opciones anteriores son:

\begin{list}{-}{}
\item \textbf{Venv}: viene incluido en la biblioteca standard de Python, no soporta Python 2.x.
\item \textbf{Virtualenv}: es la biblioteca más recomendada por los principales tutoriales de Python y se encuentran disponibles envoltorios que facilitan o implementan funcionalidades adicionales, soporta Python 2.6+ y 3.3+.
\item \textbf{Conda}: no solo es un gestor de entornos virtuales, sino que también gestiona paquetes, soporta también 2.6+ y 3.3+.
\end{list}

\subsubsection{Creación de un entorno virtual}

Para este ejemplo pondremos como ejemplos a venv y virtualenv. Las palabras que están entre `\textless ' y `\textgreater ' son variables. 

\hphantom{something\\}
{\normalsize \textbf{Venv}}

Para crear el entorno: 

\begin{lstlisting}[language=bash]
    $ python3 -m venv <DIR>
\end{lstlisting}


Para activarlo: 

\begin{lstlisting}[language=bash]
    $ source <DIR>/bin/activate
\end{lstlisting}


Para desactivarlo: 


\begin{lstlisting}[language=bash]
    $ deactivate
\end{lstlisting}


\hphantom{something\\}
{\normalsize \textbf{Virtualenv}}

Virtualenv al no estar incluido en Python deberemos instalarlo, para ello se recomienda tener un gestor de paquetes como pip instalado antes.

Para instalar virtualenv valdrá con: 

\begin{lstlisting}[language=bash]
    $ pip install virtualenv
\end{lstlisting}


Para crear el entorno:

\begin{lstlisting}[language=bash]
    $ virtualenv -p /usr/bin/python<VERSION> <DIR>
\end{lstlisting}


	
Para activarlo: sudo pip3 install -r requirements.txt

\begin{lstlisting}[language=bash]
    $ source <DIR>/bin/activate
\end{lstlisting}


Para desactivarlo:

\begin{lstlisting}[language=bash]
    $ deactivate
\end{lstlisting}


\subsection{IDE o editor de texto}

Se recomienda usar un IDE aunque un editor de texto y una línea de comandos pueden hacer la misma función. En caso de usar un IDE, hay que buscar la configuración para que pueda trabajar con el entorno virtual que hayamos creado o vayamos a crear.

Si no estamos usando un IDE, simplemente debemos entrar en el entorno virtual antes de hacer nada relacionado con la ejecución del proyecto.


\subsubsection{Importar el proyecto desde Pycharm}

Por la separación del proyecto en microservicios se recomienda el no trabajar con un solo proyecto, se recomienda importar cada servicio en un proyecto y así mantenerlos separados, esto facilita el uso de entornos virtuales.


\begin{enumerate}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\item Abrir PyCharm
\item Desde la ventana de bienvenida, hacer click en abrir y abrir la localización tfg/WhatAClass
\item Esperar a que PyCharm lea todos los archivos
\item Ir abriendo los cuadros de dialogo y comprobando su importancia
\end{enumerate}

PyCharm ya está preparado para trabajar con git, pip, entornos virtuales y docker, pero muchas veces hace falta configurar estas herramientas. Para configurarlas los cuadros de dialogo son muy utiles ya que proporcionan una buena cantidad de ayuda, facilitando la instalación de dependencias no instaladas, permitiendo tener docker como opción de ejecución predeterminada, llevando el control de versiones por ti y avisando si algo no sale como se espera.

Por último, tenemos que añadir como \emph{script} de depuración `run.py' y como \emph{script} de prueba `test.py' o la carpeta de `test/' como objetivo.

\subsubsection{Añadir características}

Para añadir características al programa debemos, en primer momento plantearnos que funcionalidad proveen, si cuadra dentro de una de las categorías que tenemos definidas (home, control de usuarios y tensorflow) o no. 

Si cuadra con alguna de las categorías definidas, nos será suficiente con añadir más rutas disponibles dentro de la blueprint de esa sección. Si no cuadra, lo que tendremos que hacer será crear esa blueprint que vayamos a implementar. 

Sea cual sea la opción que elijamos acabaremos creando funciones decoradas con `@route' o `@blueprint.route'. Estas funciones serán nuestros controladores, lo que devuelvan será lo que vea el cliente, principalmente deberíamos devolver la función `render\_template' invocada con la plantilla correspondiente. Para la internacionalización debemos importar de `flask\_babel' la función `gettext', normalmente se importa como `\_' de manera que el texto internacionalizable pasa antes por esa función.

Como ejemplo de la mayoría de conocimientos requeridos para implementar una funcionalidad nueva se expone la función de login, excesivamente comentada, a continuación.

\lstset{style=blockstyle}
\begin{lstlisting}[language=Python]
 from flask import (Blueprint, flash, render_template, 
                    url_for, redirect)
 from flask_login import current_user

 from flask_babel import gettext as _
 
 from WhatAClass.forms import LoginForm 
 from WhatAClass.models import User

 user_mng = Blueprint('user_mng', __name__)
 
 # El decorador que da una ruta de acceso
 @user_mng.route('/login', methods=['GET', 'POST'])
 def login():
 
     # Ruta desde la carpeta templates hasta el archivo
     # de la vista     
     view = 'user_mng/login.html'

     # Usuario actual
     if current_user.is_authenticated:
         # Función _ para permitir internacionalización
         flash(_('There is a logged in user already.'))
         return redirect(url_for('index.base'))

     form = LoginForm()

     # Si el formulario esta siendo subido (POST)
     if form.validate_on_submit():

         # Hacemos una petición a la base de datos
         user = User.query.filter_by(
                     email=form.email.data).first()

         # Intentamos hacer login al usuario
         if not check_and_login(
                     form.password.data, user):
                     
             return render_template(view, form=form)

         flash(_('Logged in successfully.'))
         return redirect(url_for('index.base'))

     # Salida de una petición GET
     return render_template(view, form=form)
\end{lstlisting}

Como se puede ver, lo que en otras tecnologías es una clase, lo que aquí es la \emph{blueprint}, esto hace que no sean tan útiles los diagramas uml.

Si se desea usar git para integrar el cambio, hacer un fork del proyecto y luego una pull request sería lo más adecuado. 


\subsection{Compilación}

Python es un lenguaje interpretado y no necesita compilación previa, pero algunas partes de nuestro proyecto sí que pueden necesitar ser compiladas. De momento solo las traducciones necesitan ser compiladas.

\subsubsection{Babel}

Para traducir con Babel, como ya se ha introducido anteriormente, usaremos gettext y \_. Una vez todo nuestro texto este en llamadas a esas funciones solo nos queda extraerlo para traducirlo, generar el catálogo del idioma y compilar ese catálogo para que sea de uso más eficiente.

A continuación se expondrá como se lleva a cabo este proceso:

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
    $ pybabel extract -F babel.cfg -o messages.pot WhatAClass/
\end{lstlisting}

Este primer comando lo que lleva a cabo es la extracción de todos los mensajes a ser traducidos al archivo `messages.pot'. Debemos abrir ese archivo con un editor de texto o nuestra IDE para añadir las traducciones correspondientes. Una vez estén añadidas, es hora de generar el catálogo del idioma correspondiente. 

\begin{lstlisting}[language=bash]
    $ pybabel init -i messages.pot 
                   -d WhatAClass/translations 
                   -l es
\end{lstlisting}

Con esto estamos especificando que la entrada es messages.pot, la salida estará en WhatAClass/translations y el lenguaje es el español, para otros idiomas habría que buscar sus extensiones. Por último, se compilan las traducciones para que babel pueda usarlas de manera eficiente

\begin{lstlisting}[language=bash]
    $ pybabel compile -d WhatAClass/translations
\end{lstlisting}

\begin{figure}
	\centering
	\includegraphics[width=1.0\textwidth]{translations.png}
	\caption{Proceso de traducción}\label{fig:translations.png}
\end{figure}



\subsection{Docker}

Como ya se ha hablado Docker \cite{dock, learndock, orchdock} se usa para facilitar el despliegue en cualquier servidor que lo tenga instalado. Para poder disfrutar las ventajas que nos da el uso de este servicio de virtualización debemos configurar el contenedor cómo si fuese un sistema operativo normal y corriente. Esto se lleva a cabo en un archivo llamado `Dockerfile'.

Cómo no sería rentable invertir el tiempo de instalar un sistema operativo en un contenedor, la opción que nos da Docker es el equivalente en git a hacer un fork, replica todo el sistema que pidamos y a partir de él podemos usar el sistema como queramos. Cómo se hace esto se verá a continuación explicado en las siguientes secciones.

Antes de poder ejecutar un contenedor debemos construirlo, para ello debemos situarnos en la carpeta que contenga nuestro proyecto, tras eso ejecutaremos el comando siguiente:

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
    $ docker build . [-t <name>[:<version>]]
\end{lstlisting}

Si por cualquier razón no estuviese el `Dockerfile' en la carpeta raíz del proyecto, deberíamos cambiar el `\textbf{.}' por la ruta hasta el directorio.

Para lanzar un contenedor y ponerlo en ejecución se puede hacer con el siguiente comando:

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
    $ docker run -i -t <image_id>
\end{lstlisting}

O el siguiente, si queremos usar el nombre del contenedor:

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
    $ docker run -i -t <image_name>:<version>
\end{lstlisting}

Si no nos acordásemos de que imágenes o que nombres tienen, ya que es difícil de acordarnos tanto de las ids como de los nombres auto-generados, podemos usar:

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
    $ docker images
\end{lstlisting}

Para todos estos comandos existen muchas variaciones, pero para su uso básico esta información es suficiente.


\subsubsection{Aplicación web}

En el siguiente ejemplo se muestra un archivo Dockerfile, específicamente el de la aplicación web, lo que hace cada línea se verá comentado en el propio código. 

Dado que necesita usar una base de datos que no se lanza en el mismo contenedor no podemos usar este archivo de forma individual. Si quisiésemos poder usarlo de forma individual, podríamos configurar un volumen, que es la unidad de almacenamiento persistente de docker, para almacenar una base de datos SQLite, esto se podría añadir en los archivos de configuración para que funcionase.


\lstset{style=blockstyle}
\begin{lstlisting}[language=dockerfile]
FROM tiangolo/uwsgi-nginx-flask:flask-python3.5
# De que <usuario>/<imagen>:<version> estamos`heredando'
# Es un link a dockerhub de manera implícita.

MAINTAINER Javier Martinez "javyermartinez@gmail.com"
# Quién mantiene la imagen.

WORKDIR /app
# Directorio de trabajo, 
# el equivalente a cd /app de linux

# Copiamos el código fuente 
COPY ./WhatAClass ./WhatAClass

# Copiamos la parte estática a una carpeta 
# especialmente configurada para que nginx 
# pueda servirlo más rápidamente
COPY ./WhatAClass/static ./static
# Copy es un comando que copia desde nuestra maquina y 
# directorio hasta el contenedor y directorio de trabajo.

RUN mkdir -p ./static/images
# Podemos ejecutar comandos del sistema operativo 
# con run, en este caso creamos un directorio.

# {...
# ...}

# Añadimos una variable de estado con env
ENV PYTHONPATH ``$PYTHONPATH:
                 /WhatAClass:
                 /WhatAClass/WhatAClass''

# Instalamos las dependencias
RUN pip3 install --editable .
\end{lstlisting}


\subsubsection{Tensorflow}

El contenedor de tensorflow es heredado de una instalación de tensorflow, a partir de ese punto añadimos un daemon de ssh.

En este archivo tenemos un par de comandos distintos a los del otro que explicaremos a continuación.


\begin{lstlisting}[language=dockerfile]

FROM gcr.io/tensorflow/tensorflow:latest-devel-py3
# Imagen de tensorflow y con un link explicito a grc.io

RUN apt-get update && apt-get install -y openssh-server
# Instalar el daemon de ssh

# {...
# ...}

EXPOSE 22
# Exponemos un puerto para que se pueda 
# acceder a el más facilmente

CMD ["/usr/sbin/sshd", "-D"]
# CMD es parecido a run, pero en algunos casos 
# simplifica la operación.

\end{lstlisting}


\subsubsection{Docker compose}

Para coordinar ambos contenedores en el momento que queramos ejecutar la aplicación debemos usar un servicio de orquestración, como ya estamos usando docker, la opción más fácil y, de momento, profesional es docker compose.

La forma de usar docker compose es con un archivo `docker-compose.yml'. En el se especifican todas las configuraciones adicionales que se necesiten. A continuación se mostraran un par de ejemplos del mismo.

\begin{lstlisting}[language=dockercompose]
# Versión del archivo docker compose 
# para permitir retrocompatibilidad
version: '3.0'

# Servicios que se desplegarán 
services:
  # Nombre arbitrario del servicio  
  web:
    # Directorio de construcción
    build: ./
    # Puertos
    ports:
     - "80:80"
    # Redes internas a las que puede acceder
    networks:
     - backend
    # Servicios de los que depende, no funciona
    # suficientemente bien.
    depends_on:
     - database
    # Variables de entorno
    environment:
     # Como se puede ver tras el @ accedemos a 
     # database como una dirección de red, esto
     # se debe a que su dirección interna es el 
     # nombre de su servicio.
     - DATABASE_URL=postgresql://postgres@database #{...}
  database:
    # Imagen que usamos ya que no la construimos.
    image: "postgres"
    networks:
     - backend
# Redes existentes
networks:
  backend:
\end{lstlisting}

`depends\_on' no funciona de forma correcta ya que no espera a ninguna señal del servicio del que depende. Por lo que los servicios no tienen por qué estar preparados para recibir clientes, pudiendo causar errores. Esto llega a ser un problema sobre todo en los sistemas que pueden tardar mucho en encenderse como las bases de datos.

Se ha solucionado añadiendo una espera de hasta 25 segundos en la aplicación web para la base de datos.

Otro ejemplo de docker compose algo más complejo, pero casi idéntico:


\begin{lstlisting}[language=dockercompose]
version: '3.0'
services:
  web:
    # {...}
    environment:
     - DATABASE_URL=postgresql:// # {...}
     # Se puede cambiar el despliegue de forma simple
     # mediante variables de entorno, si se mantiene la 
     # configuración.
     - WORKER_HOST_NAME=worker
     - WORKER_PORT=22
     - WORKER_USER=root
     - WORKER_PASSWORD=screencast
     - TENSORFLOW=True
    volumes:
     # Donde se monta el volumen 
     # <nombre_del_volumen>:<directorio donde se monta>
     - images:/app/static/images
 
  database:
    # {...}  
  worker:
    # {...}
networks:
  backend:
# Volúmenes compartidos
volumes:
  # Volumen con nombre
  images:
\end{lstlisting}


Una vez tenemos el archivo `docker-compose.yml' podemos usar los siguientes comandos para hacer la construcción de los contenedores y para desplegarlos:

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
    $ docker-compose build
    $ docker-compose up
\end{lstlisting}

Este proceso es lento y a no ser que queramos probar como se integran los servicios no conviene usar este método de depuración.

\section{Despliegue}

Para hacer un despliegue en un lugar en el que no este disponible el código fuente la manera más simple es instalar docker y ejecutar un comando de los dos siguientes:

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
$ wget 'URL' -O docker-compose.yml && docker-compose up
\end{lstlisting}

\lstset{style=linestyle}
\begin{lstlisting}[language=bash]
$ curl 'URL' >> docker-compose.yml && docker-compose up
\end{lstlisting}

Siendo `\textbf{https://raw.githubusercontent.com/Jazriel/TFG/master/docker-compose-hub/docker-compose.yml}' la url.
\hphantom{sin esto quedaba mal}\\

Si ya tuviésemos ese archivo descargado bastaría con ejecutar `\textbf{docker-compose up}' en el mismo directorio del archivo.

\section{Pruebas del sistema}

Las pruebas se pueden ejecutar con test.py que lanza los tests de la carpeta tests/. Si queremos ver el recubrimiento del código, podemos usar una herramienta como coverage y ejecutar test.py a traves de esa herramienta.  



\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

[No se sabe si faltan parrafos de introduccion y definiciones de conceptos o así esta bien]


\section{Idea e inicio}

Se eligió un proyecto con relación con el deep learning ya que existía un interés previo tanto a nivel personal como a nivel profesional. La gran versatilidad de este tipo de redes, ya sea en clasificación con los modelos profundos, compresión con los autoencoders o incluso generación de imágenes con las revolucionarias GAN, esta claramente demostrada, ahora queda buscarles nuevos usos que supongan una aplicación práctica más directa y útil. 

A nivel personal la importancia que tienen las redes neuronales en el mundo actual hizo que surgiese una gran curiosidad. Un ejemplo de la aplicación de este tipo de tecnología podrían ser los coches automáticos o aplicaciones surgidas últimamente, como la viral durante un par de días \href{https://www.faceapp.com/}{FaceApp}. 

A nivel profesional el deep learning se ha posicionado como uno de los algoritmos que mejor resultados han dado en muchos campos, esto abre tanto puertas profesionales y posibilidades de trabajo en industria como posibilidades de investigación y estudio en academia. 



\section{Formación}
El proyecto requería una serie de conocimientos de los que no se disponía en un principio, como es normal en cualquier proyecto no trivial. Algunos de los conocimientos necesarios se aprendieron durante la carrera pero por supuesto en un campo tan grande y cambiante como es la informática la gran mayoría de tecnologías se tuvieron que investigar y aprender por cuenta propia.

Lo necesario para llevar a cabo el proyecto se expondrá en las siguientes secciones, se intentaran omitir detalles que parezcan triviales o de muy poco interés para el lector de manera que la lectura no sea demasiado pesada.

Antes de hacer nada más lo principalmente necesario fue el conseguir un entorno con integración continua. 

Esto se presenta como una dificultad bastante grande ya que el único libro conocido sobre el tema \cite{cont07} aunque parece fiable según algunas personas es mucho más largo de lo que debería, y teniendo en cuenta que esta centrado para java se ha decidido intentar evitarlo.

Los materiales alternativos a este libro han sido  generalmente entradas de blogs y hablando con gente que lo ha usado (aunque quizá no suficientemente experimentados). \cite{fow06}
\cite{dancont}

Otras cosas que se han tenido que aprender han sido \href{https://www.tensorflow.org/}{tensorflow} y metodologías de uso de git\cite{vin10}. 

Tensorflow se estudió con los tutoriales oficiales relevantes para las partes a los que se quería dar uso. Aunque quizá no fuese necesario también se revisó la tensorflow con[youtube ref] para comprobar tanto la escalabilidad como las posibilidades más punteras en servidor y de distribución de los modelos en otros dispositivos.

Gitflow se siguió conforme a la explicación más antigua que se conoce, presentada en 2010, por Vincent Driessen. \cite{vin10}

\section{Página web y versionado de bases de datos}

No se conocía el desarrollo web con python ni con flask. Para adquirir el conocimiento sobre esto se usó los tutoriales \href{https://exploreflask.com/en/latest/}{explore flask} y \href{https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world/page/0}{megatutorial de flask}.

Hay que tener cuidado porque el primer tutorial está en python2. Esto generalmente no da casi ningún problema ya que flask se ocupa de muchos de los problemas de versión pero a la hora de usar SQLAlchemy los hash de las contraseñas en python2 se pueden guardar como string pero en python3 se deben guardar como objetos de bytes.

Una de las lecciones más importantes aprendidas de estos tutoriales es la importancia del versionado del modelo de la base de datos. Esto se hace para poder migrar de una versión a otra de forma automática. 

Es una manera bastante fácil de mantener la base de datos en buenas condiciones. Esto se debe a que si al hacer alguna migración falla, se cancela y no perdemos datos, de manera que si alguna migración falla podemos añadir lógica al como migrar para facilitar el proceso.

A parte de poder hacer versiones o `commits' a mano, podemos hacer versionado automático de la base de datos lo que infiere a partir de los modelos el cambio que ha ocurrido en la base de datos. Esta opción no se ha trabajado ya que no se ha llegado a necesitar.

\subsection{Alembic: primeros pasos}

Alembic nos permite de forma efectiva versionar las bases de datos de la misma manera que versionamos el código. Para poder empezar a usar alembic en un proyecto debemos ir a la carpeta del proyecto y ejecutar el comando:

\begin{lstlisting}[language=bash]
  $ alembic init alembic
\end{lstlisting}

Este comando creará tanto la carpeta alembic como el archivo alembic.ini, ambos colgando de la ruta desde donde hayamos ejecutado el comando. 

En la carpeta se almacenan las versiones y scripts de migración entre ellas. El archivo ini tiene la configuración de alembic y para que sepa donde tenemos la base de datos tendremos que cambiar la ruta de la base de datos a donde tengamos la nuestra. Esta ruta se guarda en el parámetro sqlalchemy.url

\begin{lstlisting}[language=python]
  >>> sqlalchemy.url = driver://user:pass@localhost/dbname
\end{lstlisting}



Para crear una revisión (commit inicial en este caso) debemos usar el comando:

\begin{lstlisting}[language=bash]
  $ alembic revision -m "message"
\end{lstlisting}

Cabe destacar que esto influye sobre todo en el mantenimiento de sistemas en producción ya que si no esta en producción los datos son más fáciles de recuperar y se pueden recurrir a alternativas menos ortodoxas (hacerlo a mano, usar scripts hechos a mano\ldots).



\section{Heroku y transición a contenedores}

Tras tener la versión básica de la página web se decidió hacer un despliegue en heroku, una de las páginas que nos permiten publicar nuestra página web dinámica. 

Heroku en concreto funciona con contenedores pero nos quita gran parte del peso del aprendizaje, la versión gratuita nos limita a un contenedor (donde dejamos nuestro proyecto como si fuera un ordenador normal) y nos deja un segundo contenedor limitado a una base de datos postgresql con unos limites de tamaño. 

Tras conseguir completar el despliegue en heroku y tener que integrar tensorflow en el proyecto, para facilitar la integración, el despliegue en otros sistema y el mantenimiento del software se decidió hacer los contenedores de forma explicita. La alternativa elegida fue docker, por ser la opción más madura. 

Para aprender a usar docker se usó la \href{https://docs.docker.com/}{documentación oficial} y el libro docker orchestration\cite{ran17}. [hacer una explicación basica de docker al estilo de la de alembic?]

\section{Docker compose y microservicios}

Docker compose sirve para organizar nuestros contenedores en despliegues concretos de manera que se puedan usar en cualguier momento, por ejemplo podríamos tener varios docker compose con distintas configuraciones según nuestras necesidades, tanto para distintos servicios de alojamiento como para distintas configuraciones de despliegue. Esto último es algo bastante complicado y que si nos propusiesemos hacerlo tendría que ser con antelación o refactorizaciones decentemente grandes.

El aprendizaje de docker compose fue sobre todo mediante la documentación oficial aunque el libro \cite{ran17} también se usó. [explicar docker compose al estilo de alembic si/no] 

Se puede desplegar con docker compose pero con una gran limitación y es que no nos permite usar varios ordenadores de la manera que los servicios de orquestración nos lo permiten.

Los microservicios como arquitectura se conocieron debido a su popularidad actual, muchos blogs han hecho artículos sobre ellos y algunos newsletter como el de O'Reilly o el de Nginx les han dado mucha importancia últimamente, adjuntando tutoriales y conferencias con las ultimas noticias.


\section{Orquestración}

Los conocidos como orquestradores nos permiten, como su nombre indica, orquestrar servicios entre varios ordenadores lo cual nos permite elegir configuraciones más versátiles y acordes a nuestras necesidades. 

Los orquestradores son docker swarm, kubernetes y Openshift. [explicar kubernetes... si/no?]

Docker swarm fue el servicio que se intentó usar debido a la existencia de secretos, que son ficheros con contraseñas o archivos que no deberían ser públicos. Estos secretos no están en docker compose, pero si en swarm aunque el tamaño del enjambre sea uno, que es equivalente a compose, lo que parece una estrategia comercial más que un diseño bien planteado.

Usar docker swarm dio gran cantidad de problemas y tras una investigación online la opinión general es que ya está listo para entornos de producción, la experiencia que podemos relatar es que swarm da problemas extra, que son difíciles de diagnosticar, quizá sea un caso concreto o que se hizo algo de manera incorrecta pero tras bastante tiempo perdido sin ser capaces de avanzar se decidió continuar con otras partes del proyecto.


\section{Conclusiones}

\subsection{Gitflow, scrum y metodologías pensadas para equipos}

Gitflow es un sistema complejo y que requiere conocimiento medio de git para poder llevarse a cabo. Realmente no se necesita para proyectos tan pequeños como los de una sola persona y el trabajo que requiere realmente se vio que no merecía la pena. Aunque parece beneficiosa para equipos no se ha podido comprobar y cabe considerar que existen criticas de la metodología en cuestión [ref].

Scrum también es parecido, se ha visto como funciona en otros entornos y es sin duda beneficioso sobre todo para equipos de tamaño mediano. Una cosa que sí que merece la pena de scrum es el tablero kanban, aunque solo seas una persona merece la pena como forma de tener claras que partes del proyecto están hechas hasta que punto.

\subsection{Tensorflow, versiones provisionales y tecnología punta}

Tras aprender tensorflow el verano pasado y no seguir usándolo tan activamente durante el primer cuatrimestre se presento un problema, tensorflow al estar en versiones de desarrollo al sacar la versión 1.0 se perdió parte del conocimiento adquirido, la api cambió, algunos de los scripts dejaron de funcionar y se desarrollaron librerías  más maduras y de más alto nivel, dejando otra parte del conocimiento adquirido parcialmente obsoleto.

Como ya se ha comentado docker swarm también dio problemas, la suposición es que se hizo algo incorrecto en docker o docker compose que hizo que swarm no funcionase correctamente. Al ser tecnología punta no hay tantas preguntas respondidas en stack overflow o foros similares sobre el tipo de problemas que nos encontramos.

Docker compose también dio problemas, más ligeros que docker swarm, con las configuraciones de red.
Suponemos lo mismo pero estos si que se pudieron arreglar.


\subsection{Microservicios}

Los microservicios como hemos hablado tienen una cantidad de aplicaciones muy diversa, y son una arquitectura muy capaz sobre todo cuando estamos proporcionando SaaS, o un sistema que necesita de muchas partes semi independientes. 

En el ámbito universitario parece que la aplicación más clara que tiene es la de que varios alumnos sean capaces de colaborar en un proyecto común. Esto se introduce ya que parece común el hecho de que varios alumnos quieran trabajar en un proyecto común y no puedan hacerlo por limitaciones en el sistema universitario, con una arquitectura de microservicios podemos facilitar que se vea la distinción entre el trabajo de cada uno de manera que superamos esa limitación. También al ser más probable que un proyecto común acabe creciendo más allá de uno individual, esta arquitectura nos ayuda a tener una transición suave sin excesivo trabajo ni demasiadas complicaciones para desplegar en producción.

Si no se piensa llevar el proyecto hasta unos niveles de complejidad elevados o mantener una cantidad no trivial de servicios durante una cantidad de tiempo media-larga no merece la pena, ya que fuerza más trabajo al equipo sin conseguir la mayoría de beneficios.